{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation With LSTM Recurrent Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = ('E:\\input.txt')\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  15640\n",
      "Total Vocab:  42\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  15490\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 150\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 42)                10794     \n",
      "=================================================================\n",
      "Total params: 274,986\n",
      "Trainable params: 274,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15490/15490 [==============================] - 285s 18ms/step - loss: 3.0171\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.01706, saving model to weights-improvement-01-3.0171.hdf5\n",
      "Epoch 2/20\n",
      "15490/15490 [==============================] - 278s 18ms/step - loss: 2.9424\n",
      "\n",
      "Epoch 00002: loss improved from 3.01706 to 2.94237, saving model to weights-improvement-02-2.9424.hdf5\n",
      "Epoch 3/20\n",
      "15490/15490 [==============================] - 278s 18ms/step - loss: 2.9313\n",
      "\n",
      "Epoch 00003: loss improved from 2.94237 to 2.93132, saving model to weights-improvement-03-2.9313.hdf5\n",
      "Epoch 4/20\n",
      "15490/15490 [==============================] - 279s 18ms/step - loss: 2.9202\n",
      "\n",
      "Epoch 00004: loss improved from 2.93132 to 2.92020, saving model to weights-improvement-04-2.9202.hdf5\n",
      "Epoch 5/20\n",
      "15490/15490 [==============================] - 282s 18ms/step - loss: 2.8934\n",
      "\n",
      "Epoch 00005: loss improved from 2.92020 to 2.89337, saving model to weights-improvement-05-2.8934.hdf5\n",
      "Epoch 6/20\n",
      "15490/15490 [==============================] - 282s 18ms/step - loss: 2.8445\n",
      "\n",
      "Epoch 00006: loss improved from 2.89337 to 2.84449, saving model to weights-improvement-06-2.8445.hdf5\n",
      "Epoch 7/20\n",
      "15490/15490 [==============================] - 283s 18ms/step - loss: 2.8252\n",
      "\n",
      "Epoch 00007: loss improved from 2.84449 to 2.82520, saving model to weights-improvement-07-2.8252.hdf5\n",
      "Epoch 8/20\n",
      "15490/15490 [==============================] - 284s 18ms/step - loss: 2.8015\n",
      "\n",
      "Epoch 00008: loss improved from 2.82520 to 2.80146, saving model to weights-improvement-08-2.8015.hdf5\n",
      "Epoch 9/20\n",
      "15490/15490 [==============================] - 271s 17ms/step - loss: 2.7893\n",
      "\n",
      "Epoch 00009: loss improved from 2.80146 to 2.78929, saving model to weights-improvement-09-2.7893.hdf5\n",
      "Epoch 10/20\n",
      "15490/15490 [==============================] - 283s 18ms/step - loss: 2.7765\n",
      "\n",
      "Epoch 00010: loss improved from 2.78929 to 2.77646, saving model to weights-improvement-10-2.7765.hdf5\n",
      "Epoch 11/20\n",
      "15490/15490 [==============================] - 288s 19ms/step - loss: 2.7664\n",
      "\n",
      "Epoch 00011: loss improved from 2.77646 to 2.76639, saving model to weights-improvement-11-2.7664.hdf5\n",
      "Epoch 12/20\n",
      "15490/15490 [==============================] - 269s 17ms/step - loss: 2.7579\n",
      "\n",
      "Epoch 00012: loss improved from 2.76639 to 2.75788, saving model to weights-improvement-12-2.7579.hdf5\n",
      "Epoch 13/20\n",
      "15490/15490 [==============================] - 278s 18ms/step - loss: 2.7420\n",
      "\n",
      "Epoch 00013: loss improved from 2.75788 to 2.74202, saving model to weights-improvement-13-2.7420.hdf5\n",
      "Epoch 14/20\n",
      "15490/15490 [==============================] - 279s 18ms/step - loss: 2.7840\n",
      "\n",
      "Epoch 00014: loss did not improve\n",
      "Epoch 15/20\n",
      "15490/15490 [==============================] - 280s 18ms/step - loss: 2.7268\n",
      "\n",
      "Epoch 00015: loss improved from 2.74202 to 2.72680, saving model to weights-improvement-15-2.7268.hdf5\n",
      "Epoch 16/20\n",
      "15490/15490 [==============================] - 280s 18ms/step - loss: 2.7105\n",
      "\n",
      "Epoch 00016: loss improved from 2.72680 to 2.71053, saving model to weights-improvement-16-2.7105.hdf5\n",
      "Epoch 17/20\n",
      "15490/15490 [==============================] - 287s 19ms/step - loss: 2.6969\n",
      "\n",
      "Epoch 00017: loss improved from 2.71053 to 2.69692, saving model to weights-improvement-17-2.6969.hdf5\n",
      "Epoch 18/20\n",
      "15490/15490 [==============================] - 286s 18ms/step - loss: 2.6811\n",
      "\n",
      "Epoch 00018: loss improved from 2.69692 to 2.68114, saving model to weights-improvement-18-2.6811.hdf5\n",
      "Epoch 19/20\n",
      "15490/15490 [==============================] - 281s 18ms/step - loss: 2.6682\n",
      "\n",
      "Epoch 00019: loss improved from 2.68114 to 2.66820, saving model to weights-improvement-19-2.6682.hdf5\n",
      "Epoch 20/20\n",
      "15490/15490 [==============================] - 280s 18ms/step - loss: 2.6501\n",
      "\n",
      "Epoch 00020: loss improved from 2.66820 to 2.65007, saving model to weights-improvement-20-2.6501.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29f96d99a90>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-2.6501.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" and tasty packed well and arrive in a timely manner\n",
      "i bought these for my husband who is currently overseas he loves these and apparently his staff li \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the bod the toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "71084/71084 [==============================] - 1537s 22ms/step - loss: 2.4902\n",
      "\n",
      "Epoch 00001: loss improved from 2.57449 to 2.49017, saving model to weights-improvement-01-2.4902-bigger.hdf5\n",
      "Epoch 2/30\n",
      "71084/71084 [==============================] - 1591s 22ms/step - loss: 2.4146\n",
      "\n",
      "Epoch 00002: loss improved from 2.49017 to 2.41456, saving model to weights-improvement-02-2.4146-bigger.hdf5\n",
      "Epoch 3/30\n",
      "71084/71084 [==============================] - 1649s 23ms/step - loss: 2.3435\n",
      "\n",
      "Epoch 00003: loss improved from 2.41456 to 2.34350, saving model to weights-improvement-03-2.3435-bigger.hdf5\n",
      "Epoch 4/30\n",
      "71084/71084 [==============================] - 4222s 59ms/step - loss: 2.2778\n",
      "\n",
      "Epoch 00004: loss improved from 2.34350 to 2.27783, saving model to weights-improvement-04-2.2778-bigger.hdf5\n",
      "Epoch 5/30\n",
      "71084/71084 [==============================] - 1537s 22ms/step - loss: 2.2158\n",
      "\n",
      "Epoch 00005: loss improved from 2.27783 to 2.21575, saving model to weights-improvement-05-2.2158-bigger.hdf5\n",
      "Epoch 6/30\n",
      "71084/71084 [==============================] - 1582s 22ms/step - loss: 2.1632\n",
      "\n",
      "Epoch 00006: loss improved from 2.21575 to 2.16320, saving model to weights-improvement-06-2.1632-bigger.hdf5\n",
      "Epoch 7/30\n",
      "71084/71084 [==============================] - 2147s 30ms/step - loss: 2.1110\n",
      "\n",
      "Epoch 00007: loss improved from 2.16320 to 2.11102, saving model to weights-improvement-07-2.1110-bigger.hdf5\n",
      "Epoch 8/30\n",
      "71084/71084 [==============================] - 1579s 22ms/step - loss: 2.0650\n",
      "\n",
      "Epoch 00008: loss improved from 2.11102 to 2.06499, saving model to weights-improvement-08-2.0650-bigger.hdf5\n",
      "Epoch 9/30\n",
      "71084/71084 [==============================] - 1606s 23ms/step - loss: 2.0142\n",
      "\n",
      "Epoch 00009: loss improved from 2.06499 to 2.01423, saving model to weights-improvement-09-2.0142-bigger.hdf5\n",
      "Epoch 10/30\n",
      "71084/71084 [==============================] - 1580s 22ms/step - loss: 1.9717\n",
      "\n",
      "Epoch 00010: loss improved from 2.01423 to 1.97169, saving model to weights-improvement-10-1.9717-bigger.hdf5\n",
      "Epoch 11/30\n",
      "71084/71084 [==============================] - 1633s 23ms/step - loss: 1.9335\n",
      "\n",
      "Epoch 00011: loss improved from 1.97169 to 1.93353, saving model to weights-improvement-11-1.9335-bigger.hdf5\n",
      "Epoch 12/30\n",
      "71084/71084 [==============================] - 3367s 47ms/step - loss: 1.8903\n",
      "\n",
      "Epoch 00012: loss improved from 1.93353 to 1.89030, saving model to weights-improvement-12-1.8903-bigger.hdf5\n",
      "Epoch 13/30\n",
      "71084/71084 [==============================] - 53626s 754ms/step - loss: 1.8540\n",
      "\n",
      "Epoch 00013: loss improved from 1.89030 to 1.85397, saving model to weights-improvement-13-1.8540-bigger.hdf5\n",
      "Epoch 14/30\n",
      "71084/71084 [==============================] - 2357s 33ms/step - loss: 1.8179\n",
      "\n",
      "Epoch 00014: loss improved from 1.85397 to 1.81788, saving model to weights-improvement-14-1.8179-bigger.hdf5\n",
      "Epoch 15/30\n",
      "71084/71084 [==============================] - 2970s 42ms/step - loss: 1.7880\n",
      "\n",
      "Epoch 00015: loss improved from 1.81788 to 1.78805, saving model to weights-improvement-15-1.7880-bigger.hdf5\n",
      "Epoch 16/30\n",
      "71084/71084 [==============================] - 2731s 38ms/step - loss: 1.7532\n",
      "\n",
      "Epoch 00016: loss improved from 1.78805 to 1.75322, saving model to weights-improvement-16-1.7532-bigger.hdf5\n",
      "Epoch 17/30\n",
      "71084/71084 [==============================] - 2131s 30ms/step - loss: 1.7208\n",
      "\n",
      "Epoch 00017: loss improved from 1.75322 to 1.72079, saving model to weights-improvement-17-1.7208-bigger.hdf5\n",
      "Epoch 18/30\n",
      "71084/71084 [==============================] - 1580s 22ms/step - loss: 1.6881\n",
      "\n",
      "Epoch 00018: loss improved from 1.72079 to 1.68806, saving model to weights-improvement-18-1.6881-bigger.hdf5\n",
      "Epoch 19/30\n",
      "71084/71084 [==============================] - 1563s 22ms/step - loss: 1.6612\n",
      "\n",
      "Epoch 00019: loss improved from 1.68806 to 1.66119, saving model to weights-improvement-19-1.6612-bigger.hdf5\n",
      "Epoch 20/30\n",
      "71084/71084 [==============================] - 3232s 45ms/step - loss: 1.6395\n",
      "\n",
      "Epoch 00020: loss improved from 1.66119 to 1.63947, saving model to weights-improvement-20-1.6395-bigger.hdf5\n",
      "Epoch 21/30\n",
      "71084/71084 [==============================] - 1569s 22ms/step - loss: 1.6124\n",
      "\n",
      "Epoch 00021: loss improved from 1.63947 to 1.61242, saving model to weights-improvement-21-1.6124-bigger.hdf5\n",
      "Epoch 22/30\n",
      "71084/71084 [==============================] - 1653s 23ms/step - loss: 1.5855\n",
      "\n",
      "Epoch 00022: loss improved from 1.61242 to 1.58545, saving model to weights-improvement-22-1.5855-bigger.hdf5\n",
      "Epoch 23/30\n",
      "71084/71084 [==============================] - 1641s 23ms/step - loss: 1.5672\n",
      "\n",
      "Epoch 00023: loss improved from 1.58545 to 1.56721, saving model to weights-improvement-23-1.5672-bigger.hdf5\n",
      "Epoch 24/30\n",
      "71084/71084 [==============================] - 1629s 23ms/step - loss: 1.5453\n",
      "\n",
      "Epoch 00024: loss improved from 1.56721 to 1.54529, saving model to weights-improvement-24-1.5453-bigger.hdf5\n",
      "Epoch 25/30\n",
      "71084/71084 [==============================] - 1639s 23ms/step - loss: 1.5235\n",
      "\n",
      "Epoch 00025: loss improved from 1.54529 to 1.52345, saving model to weights-improvement-25-1.5235-bigger.hdf5\n",
      "Epoch 26/30\n",
      "71084/71084 [==============================] - 1658s 23ms/step - loss: 1.5060\n",
      "\n",
      "Epoch 00026: loss improved from 1.52345 to 1.50597, saving model to weights-improvement-26-1.5060-bigger.hdf5\n",
      "Epoch 27/30\n",
      "71084/71084 [==============================] - 1653s 23ms/step - loss: 1.4909\n",
      "\n",
      "Epoch 00027: loss improved from 1.50597 to 1.49095, saving model to weights-improvement-27-1.4909-bigger.hdf5\n",
      "Epoch 28/30\n",
      "71084/71084 [==============================] - 1640s 23ms/step - loss: 1.4742\n",
      "\n",
      "Epoch 00028: loss improved from 1.49095 to 1.47419, saving model to weights-improvement-28-1.4742-bigger.hdf5\n",
      "Epoch 29/30\n",
      "71084/71084 [==============================] - 1594s 22ms/step - loss: 1.4533\n",
      "\n",
      "Epoch 00029: loss improved from 1.47419 to 1.45333, saving model to weights-improvement-29-1.4533-bigger.hdf5\n",
      "Epoch 30/30\n",
      "71084/71084 [==============================] - 1560s 22ms/step - loss: 1.4392\n",
      "\n",
      "Epoch 00030: loss improved from 1.45333 to 1.43922, saving model to weights-improvement-30-1.4392-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a7878d6550>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=30, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-30-1.4392-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  related to formula changes in the past. unfortunately, i now need to find a new food that my cats will eat.\n",
      "good flavor! these came securely packed.. \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i would nut can iete and brown sugar on the same for a long come dook aod i wes really love then all of the taste is ooe of the seeeneo and it sastes tian the steat fren saste great. the price ard seally gat a bate of the sarte in the same for a long come dook and i wes a sially orocuct for a long \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# generate characters\n",
    "for i in range(300):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B - Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Activation function from Softmax to ReLU\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='relu'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 42)                10794     \n",
      "=================================================================\n",
      "Total params: 274,986\n",
      "Trainable params: 274,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15490/15490 [==============================] - 295s 19ms/step - loss: 3.8820\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.88196, saving model to weights-improvement-01-3.8820.hdf5\n",
      "Epoch 2/20\n",
      "15490/15490 [==============================] - 295s 19ms/step - loss: 3.5455\n",
      "\n",
      "Epoch 00002: loss improved from 3.88196 to 3.54549, saving model to weights-improvement-02-3.5455.hdf5\n",
      "Epoch 3/20\n",
      " 2432/15490 [===>..........................] - ETA: 4:12 - loss: 3.5466"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-30-1.4392-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# generate characters\n",
    "for i in range(300):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C - Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "# Changing Cost function from crossentropy to Hinge\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='hinge', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-30-1.4392-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='hinge', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# generate characters\n",
    "for i in range(300):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D - Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "# Changing Epoch and Batch size from 20 & 128 to 30 & 64\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=30, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-30-1.4392-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# generate characters\n",
    "for i in range(300):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E - Gradient Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "# Changing Gradient Estimation from Adam to Adadelta\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-30-1.4392-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# generate characters\n",
    "for i in range(300):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F - Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "#Changing the network architecture by adding one more hidden layer with ReLU activation function, changing the probability to 015 from 0.20, changing memory units from 256 to 100\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-30-1.4392-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# generate characters\n",
    "for i in range(300):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G - Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.initializers.Initializer()\n",
    "keras.initializers.Zeros()\n",
    "keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "# Changing the weight and bias as per our wish; weight as random unifrom and bias as zero\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax',kernel_initializer='random_uniform',bias_initializer='zeros' ))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-30-1.4392-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# generate characters\n",
    "for i in range(300):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
